#!/bin/bash
for i in {1..5}
do
   echo "Welcome $i times"
done


function get_sku_id_in_each_day {
    SAVE_PATH=$2
    hadoop fs -test -e $SAVE_PATH
    if [ $? -eq 0 ];then
         hadoop fs -rmr ${SAVE_PATH}
    fi
    hadoop fs -mkdir ${SAVE_PATH}
    for i in {0..-1}
    do
        j=$[-1*${i}]
        day=`date -d "${i} days ago" +%Y-%m-%d`
        if [ $i -lt 10 ]; then
            sku_input="/user/part-0000${j}"
            sku_output="/user/${day}"
            echo ${sku_input}
            echo ${sku_output}
        fi
        hadoop fs -mkdir ${sku_output}
        hadoop fs -cp ${sku_input} ${sku_output}
    done
}
   

[root@showsky ~]# date -d "4 days ago" +%Y%m%d
20100103
上面是四天前的日期
[root@showsky ~]# date -d "-1 days ago" +%Y%m%d
20100108
上面是一天后的日期


check system time
var=`date "+%Y-%m-%d %H:%M:%S"` 


check path exist or not:
hadoop fs -test -e ${SAVE_PATH}
 if [ $? -eq 0 ]
 then
     hadoop fs -rmr ${SAVE_PATH}
 fi


shh to other linux machine and execute the command: run the t.sh in machine 172.20.XXX.XX
#!/usr/bin/expect
set timeout 30
spawn ssh admin@172.20.XXX.XX
expect "admin@172.20.XXX.XX's password:"
send "password\r"
sleep 0.2
send "cd mypath/directory/sub\n"
send "./t.sh\n"
sleep 0.2
interact



check the partation exist or not:
function check_partition(){
   #table,dt
   temp=`hive -e "show partitions $1"`
   echo $temp|grep -wq "$2"
   if [ $? -eq 0 ];then
       echo "ok"
       return 0
    else
       return 1
    fi  
}
